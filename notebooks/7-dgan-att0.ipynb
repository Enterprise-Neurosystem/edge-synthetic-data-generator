{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea6162fe-96d2-48c9-9cd0-27a8863333d1",
   "metadata": {},
   "source": [
    "https://github.com/gretelai/gretel-synthetics/blob/master/examples/timeseries_dgan.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b628f5e1-5982-4b89-b170-65205f2b7ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# version recommended by source\n",
    "!pip install torch==1.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfe71ee-35f0-458f-a15b-d40e81e21cd0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/gretelai/gretel-synthetics.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144226a-f7df-4bc4-85c3-410679af5d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pickle import dump, load\n",
    "\n",
    "import torch\n",
    "\n",
    "from gretel_synthetics.timeseries_dgan.dgan import DGAN\n",
    "from gretel_synthetics.timeseries_dgan.config import DGANConfig, OutputType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc758cca-a4bc-4c54-bfed-b960a44d05df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/xiaxiaxu/predictmachinefailureinadvance/data\n",
    "sensor = pd.read_csv(\"sensor.csv\")\n",
    "\n",
    "# data columns\n",
    "COLS = [\"sensor_25\", \"sensor_11\", \"sensor_36\", \"sensor_34\", \"machine_status\"]\n",
    "\n",
    "# only keeping cols w high var in pca analysis + machine status\n",
    "data = sensor[[\"timestamp\"] + COLS]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38af530-7af6-4c96-aa48-172334398948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets convert our datatypes to the correct ones\n",
    "\n",
    "print(data.dtypes, \"\\n\")\n",
    "# i have no idea why i have to run this line twice for it to work.\n",
    "data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc7078-fc5f-4f88-8b54-fcbebc17bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, what can our machine status be?\n",
    "\n",
    "# data = data.convert_dtypes()\n",
    "print(data.dtypes, \"\\n\")\n",
    "# i have no idea why i have to run this line twice for it to work.\n",
    "data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n",
    "\n",
    "data.machine_status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e12bb-3f7d-4c6d-adb6-5f1a5bdb2015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 4 sensors\n",
    "\n",
    "for c in COLS:\n",
    "    if c == \"machine_status\":\n",
    "        continue\n",
    "    plt.plot(data[\"timestamp\"], data[c], label=c)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.ylabel(\"Sensor Value\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100795fe-cef4-4f3b-9df3-935f201dadf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many NaN rows do we have?\n",
    "\n",
    "print(len(data[data.isna().any(axis=1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7021da9d-37df-4a40-a035-6933b6a03a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets drop them : )\n",
    "\n",
    "data.dropna(axis=0, inplace=True)\n",
    "\n",
    "print(len(data[data.isna().any(axis=1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1975505-a81a-4762-8125-7c6bdebb697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try only using data centered around 2 failures.\n",
    "\n",
    "data_around_failures = data.iloc[16000:26080]\n",
    "print(len(data_around_failures))\n",
    "data_around_failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686b7753-b07d-408a-97a9-461516772342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 4 sensors\n",
    "\n",
    "for c in COLS:\n",
    "    if c == \"machine_status\":\n",
    "        continue\n",
    "    plt.plot(data_around_failures[\"timestamp\"], data_around_failures[c], label=c)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.ylabel(\"Sensor Value\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083d817d-7c4e-4030-ba38-c28f94cb850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now scale our data between 0-1\n",
    "\n",
    "scaler = MinMaxScaler((0, 1))\n",
    "sensor_cols = [\"sensor_25\", \"sensor_11\", \"sensor_36\", \"sensor_34\"]\n",
    "\n",
    "# scaling our data, then saving our scaler object for future use.\n",
    "data_around_failures[sensor_cols] = scaler.fit_transform(\n",
    "    data_around_failures[sensor_cols]\n",
    ")\n",
    "\n",
    "dump(scaler, open(\"dGAN_scaler.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07428041-d3ef-4989-8c42-dc74fc418985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 4 scaled sensors\n",
    "\n",
    "for c in COLS:\n",
    "    if c == \"machine_status\":\n",
    "        continue\n",
    "    plt.plot(data_around_failures[\"timestamp\"], data_around_failures[c], label=c)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.ylabel(\"Sensor Value\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0653b0-2323-4161-a446-a1b4760b786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check - let's unscale the data and replot\n",
    "\n",
    "sc = load(open(\"dGAN_scaler.pkl\", \"rb\"))\n",
    "\n",
    "unscaled_data = sc.inverse_transform(data_around_failures[sensor_cols])\n",
    "\n",
    "unscaled_data_df = pd.DataFrame(unscaled_data, columns=sensor_cols)\n",
    "unscaled_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9718a7bf-72b9-44a4-b28c-43b83b62323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in COLS:\n",
    "    if c == \"machine_status\":\n",
    "        continue\n",
    "    plt.plot(data_around_failures[\"timestamp\"], unscaled_data_df[c], label=c)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.ylabel(\"Sensor Value\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2241974-62bb-4bcd-b938-107e21401747",
   "metadata": {},
   "source": [
    "okay, scaling sanity check passed. lets start splitting the data to prepare it for training in dGAN.\n",
    "\n",
    "NOTE: data generated will be in the shape of data passed in for training. so if we pass in samples of 10 rows per, it will only generate a sample of 10 rows.\n",
    "\n",
    "we have 10,080 rows of data now, which corresponds to 10,080 minutes of data = 168 hours of data = 7 days of data.\n",
    "\n",
    "I'm going to try splitting the data into two hour segments - giving us 84 120row/minute readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743f171-ded4-47c9-ba61-90d82319192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping timestamp col\n",
    "features = data_around_failures.drop(columns=[\"timestamp\", \"machine_status\"]).to_numpy()\n",
    "print(features.shape)\n",
    "\n",
    "# number of samples to split into\n",
    "n = features.shape[0] // 120\n",
    "print(n)\n",
    "\n",
    "# reshape the data accordingly\n",
    "features = features[: (n * 120), :].reshape(-1, 120, features.shape[1])\n",
    "# Shape is now (# examples, # time points, # features)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3db10-2461-4b53-b7c0-4393d426af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few of the 2-hour training samples\n",
    "# note x-axis isnt accurate in these plots.\n",
    "xaxis_2hr = data_around_failures[\"timestamp\"][0:120]\n",
    "\n",
    "\n",
    "def plot_hours(f):\n",
    "    for i, c in enumerate(sensor_cols):\n",
    "        plt.plot(xaxis_2hr, f[:, i], label=c)\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(md.HourLocator(byhour=range(2, 24, 3)))\n",
    "    ax.xaxis.set_major_formatter(md.DateFormatter(\"%H:%M\"))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Sensor Readings\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_hours(features[80, :, :])\n",
    "plot_hours(features[3, :, :])\n",
    "plot_hours(features[21, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e6261-714c-4a63-9c07-ecf79f826c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended to train with a GPU - am not for now.\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f69d93d-66d2-4888-8828-bb6eae9fa9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up DGAN config.\n",
    "\n",
    "config = DGANConfig(\n",
    "    max_sequence_len=features.shape[1],\n",
    "    sample_len=12,\n",
    "    batch_size=min(1000, features.shape[0]),\n",
    "    apply_feature_scaling=False,  # already scaled\n",
    "    apply_example_scaling=False,\n",
    "    use_attribute_discriminator=False,\n",
    "    generator_learning_rate=1e-4,\n",
    "    discriminator_learning_rate=1e-4,\n",
    "    epochs=10000,\n",
    ")\n",
    "\n",
    "model = DGAN(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca8415-f802-42ce-91f9-638c8d6aaa56",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train_numpy(\n",
    "    features,\n",
    "    feature_types=[OutputType.CONTINUOUS] * features.shape[2],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150da97-1b52-4c27-b87e-1e2d4161187b",
   "metadata": {},
   "source": [
    "finished training at around 6-7 mins on large notebook image - not bad! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68bca9c-66c7-4b8a-947b-b98fa4bc7fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data - this ran near instantly\n",
    "_, synthetic_features = model.generate_numpy(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753f4a8a-e074-4a18-b8ee-925f7068f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some synthetic 2-hour samples\n",
    "plot_hours(synthetic_features[10, :, :])\n",
    "plot_hours(synthetic_features[42, :, :])\n",
    "plot_hours(synthetic_features[6, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4c7ea-3c51-4bff-99b5-6544dcaedba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare (non-temporal) correlations between the 4 sensors\n",
    "synthetic_df = pd.DataFrame(\n",
    "    synthetic_features.reshape(-1, synthetic_features.shape[2]), columns=sensor_cols\n",
    ")\n",
    "\n",
    "print(\"Correlation in real data:\")\n",
    "print(data_around_failures.drop(columns=[\"timestamp\", \"machine_status\"]).corr())\n",
    "print()\n",
    "print(\"Correlation in synthetic data:\")\n",
    "print(synthetic_df.corr())\n",
    "\n",
    "# Correlations between sensor variables are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacab6de-0b7b-41e8-8cee-f0c32640e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distribution of sensor_34 values\n",
    "plt.hist(\n",
    "    [features[:, :, 3].flatten(), synthetic_features[:, :, 3].flatten()],\n",
    "    label=[\"real\", \"synthetic\"],\n",
    "    bins=25,\n",
    "    density=True,\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Sensor 34 Values\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f4f264-0223-4396-b5c7-7fec27e12260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distribution of sensor_25 values\n",
    "plt.hist(\n",
    "    [features[:, :, 0].flatten(), synthetic_features[:, :, 0].flatten()],\n",
    "    label=[\"real\", \"synthetic\"],\n",
    "    bins=25,\n",
    "    density=True,\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Sensor 25 Values\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e1636-c2f6-4855-8203-618126d0bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distribution of sensor_11 values\n",
    "plt.hist(\n",
    "    [features[:, :, 1].flatten(), synthetic_features[:, :, 1].flatten()],\n",
    "    label=[\"real\", \"synthetic\"],\n",
    "    bins=25,\n",
    "    density=True,\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Sensor 11 Values\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc9889-8551-4d31-b624-ab29892a730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distribution of sensor_36 values\n",
    "plt.hist(\n",
    "    [features[:, :, 2].flatten(), synthetic_features[:, :, 2].flatten()],\n",
    "    label=[\"real\", \"synthetic\"],\n",
    "    bins=25,\n",
    "    density=True,\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Sensor 36 Values\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ff8a5-d321-42ab-bcb0-a201fc9564f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model for future use\n",
    "model.save(\"dgan_model_0.pt\")\n",
    "\n",
    "# X = model.load(\"dgan_model_0.pt\")\n",
    "\n",
    "# X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f8095-3072-4c77-bc66-33eaf9ab902c",
   "metadata": {},
   "source": [
    "### NOTES ON RESULTS\n",
    "\n",
    "- this is really promising!! train time was fast, data generation is near instantaneous and overall the synthetic data looks solid to the eye test and simple metrics and plots.\n",
    "\n",
    "- in future attempts, I'll be passing in more data to the model, likely will be using longer samples and more of them, either by selecting a larger initial slice of the data, or by using overlapping windows, or both! We'll see how that affects train time and performance. \n",
    "\n",
    "- will need to discuss with team how long the samples generated should be ideally. if using longer samples (like days of data) is needed, we could try thinning the data out again and using every 2 or 3 rows if needed.\n",
    "\n",
    "- also, I scaled the data manually before but it looks like we could pass in the data without scaling and then it would scale the data for training and then unscale the data when generating synthetic data - could be convenient if it works well. \n",
    "\n",
    "- could also try passing in 'machine_status' and seeing how that changes results as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
