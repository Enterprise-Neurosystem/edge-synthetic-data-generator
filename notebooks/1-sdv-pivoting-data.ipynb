{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa2369-aeb0-4013-8899-c68696fa88af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sdv==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ebd4c-61d6-44c8-b119-1c97afce783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.timeseries import PAR\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5985773d-a817-4582-9a83-d6becc48eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/xiaxiaxu/predictmachinefailureinadvance/data\n",
    "sensor = pd.read_csv(\"sensor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe0f3c8-758b-4df7-93e8-3d4e9500a04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keeping cols w high var in pca analysis + machine status\n",
    "data = sensor[\n",
    "    [\"timestamp\", \"sensor_25\", \"sensor_11\", \"sensor_36\", \"sensor_34\", \"machine_status\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8550ce-ac03-40a8-8c7f-348f2bc1f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, what can our machine status be?\n",
    "\n",
    "data = data.convert_dtypes()\n",
    "print(data.dtypes, \"\\n\")\n",
    "data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n",
    "\n",
    "data.machine_status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e97a73-9f4c-4285-b681-e9386f58cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"machine_status\"] == \"BROKEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34741c2b-107e-4a32-ad69-d024029c9786",
   "metadata": {},
   "source": [
    "Alright, we've got 7 broken instances. The first two are within ~7,000 rows, lets select the 10,000 rows around them.\n",
    "\n",
    "Then we'll scale the data with minmaxscaler (should be able to apply that directly to our dataframe) then pivot our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72598feb-7644-4a02-a390-6a8645b5f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_around_failures = data.iloc[16000:26000]\n",
    "len(data_around_failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa4ead9-6435-4513-bb6c-41d64427c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_around_failures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a70096-4221-4de0-82b9-96f11870469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_around_failures[data_around_failures.isna().any(axis=1)])\n",
    "\n",
    "# we only have one row with NA, let's simply drop that.\n",
    "\n",
    "data_around_failures.dropna(axis=0, inplace=True)\n",
    "\n",
    "print(data_around_failures[data_around_failures.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67e50f-4221-4c2f-aa29-f15071846adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the machine status variable.\n",
    "# doing it manually, its pretty simple\n",
    "\n",
    "cleanup_nums = {\"machine_status\": {\"NORMAL\": 0, \"BROKEN\": 1, \"RECOVERING\": 2}}\n",
    "data_around_failures = data_around_failures.replace(cleanup_nums)\n",
    "data_around_failures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc22c8d1-0fce-4001-bf05-7f03e66613d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "sensor_cols = [\"sensor_25\", \"sensor_11\", \"sensor_36\", \"sensor_34\"]\n",
    "\n",
    "# scaling our data, then saving our scaler object for future use.\n",
    "data_around_failures[sensor_cols] = scaler.fit_transform(\n",
    "    data_around_failures[sensor_cols]\n",
    ")\n",
    "\n",
    "dump(scaler, open(\"scaler.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483cc6a-fa32-4ea7-ab54-023ec445b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_around_failures.shape)\n",
    "\n",
    "data_around_failures.head()\n",
    "\n",
    "# now our data is scaled to be within 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef21a4da-db88-4725-a3a5-0bebf792cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's try 'melting' our data\n",
    "\n",
    "melted = data_around_failures.melt(\"timestamp\")\n",
    "melted.sort_values(by=\"timestamp\", inplace=True)\n",
    "melted.reset_index(inplace=True)\n",
    "melted = melted.drop(\"index\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad60b63-8aff-4ba4-8d01-5bb62f9e97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(melted.shape)\n",
    "melted.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf868d5-fb85-4a9c-9564-5f1bad528dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check, pivoting our melted data back to see how that works.\n",
    "\n",
    "pivoted = melted.pivot(index=\"timestamp\", columns=\"variable\", values=\"value\")\n",
    "\n",
    "print(pivoted.shape)\n",
    "pivoted.head(10)\n",
    "\n",
    "# ok looks fine to me, we're missing an index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa11ae3-c68e-403f-af4e-fc32a91830c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the 'variable' variable (not my best name)\n",
    "# doing it manually, its pretty simple\n",
    "\n",
    "# encoding them as strings not as ints - think that ints have messed things up\n",
    "\n",
    "cleanup_nums_var = {\n",
    "    \"variable\": {\n",
    "        \"machine_status\": \"0\",\n",
    "        \"sensor_25\": \"1\",\n",
    "        \"sensor_34\": \"2\",\n",
    "        \"sensor_11\": \"3\",\n",
    "        \"sensor_36\": \"4\",\n",
    "    }\n",
    "}\n",
    "melted = melted.replace(cleanup_nums_var)\n",
    "melted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026be894-cbbf-4b77-a1c8-4b14423a3558",
   "metadata": {},
   "outputs": [],
   "source": [
    "melted[\"variable\"] = melted.variable.astype(\"str\")\n",
    "\n",
    "melted[\"variable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3075ca-c15d-402d-8f6e-7ec180c59e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, we have selected data around 2 anomalies, we have scaled our numerical values\n",
    "# and we have melted our data to be 'longer' rather than 'wider'\n",
    "# also am leaving in the machine_status variable, perhaps will help with training : )\n",
    "\n",
    "sequence_index = \"timestamp\"\n",
    "entity_columns = [\"variable\"]\n",
    "\n",
    "model = PAR(\n",
    "    sequence_index=sequence_index,\n",
    "    entity_columns=entity_columns,\n",
    "    verbose=True,\n",
    "    epochs=64,\n",
    ")\n",
    "\n",
    "\n",
    "print(melted.dtypes, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27527d7-f4ea-4b7b-926f-9e144f292fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(melted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4050f51-74a0-4aea-964d-a16ac466c87c",
   "metadata": {},
   "source": [
    "ok this took ~24 mins on a large notebook image.\n",
    "\n",
    "trained for 64 epochs as 128 seemed to greatly overfit, however im not sure what their loss metric is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfd1369-4b29-4a1a-b6f8-77c3032a2c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"melted_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6c8a1-6171-4f7b-9efa-ae7c5c29aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "melted.variable.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
